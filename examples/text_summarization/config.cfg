[main]

#home directory:
home_dir = /home/ubuntu/mnt/training_results/

# Directory to cache the tokenizer. 
cache_dir= %(home_dir)s/tokenizer_cache/

# Directory to download the preprocessed data.
data_dir= %(home_dir)s/downloaded_data/

# Directory to save the output model and prediction results.
output_dir= %(home_dir)s/output/

# Transformer model used in the extractive summarization
model_name=bert-base-german-dbmdz-uncased

# batch size in terms of the number of samples in training
batch_size=10

# Maximum number of training steps run in training. If quick_run is set, this is ignored
max_steps= 100000

# Warm-up number of training steps run in training. If quick_run is set, this is ignored
warmup_steps=5000

# Number of sentences selected in prediction for evaluation.
top_n=3

# Summary file name generated by prediction for evaluation.
summary_filename=

# training data file which is saved through torch 
train_file= 

# test data file for evaluation.
test_file=

# Whether to have a quick run
quick_run= 

# Encoder types in the extractive summarizer. â€”> ["baseline", "classifier", "transformer", "rnn"],
encoder=

# training learning rate
learning_rate=

# model file name saved for evaluation.
model_filename=